{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! pip install mnist\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "def get_mnist_threes_nines():\n",
    "    # 80/20 train test split\n",
    "    Y0 = 3\n",
    "    Y1 = 9\n",
    "\n",
    "    y_train = mnist.train_labels()\n",
    "    y_test = mnist.test_labels()\n",
    "    X_train = (mnist.train_images()/255.0)\n",
    "    X_test = (mnist.test_images()/255.0)\n",
    "    train_idxs = np.logical_or(y_train == Y0, y_train == Y1)\n",
    "    test_idxs = np.logical_or(y_test == Y0, y_test == Y1)\n",
    "    y_train = y_train[train_idxs].astype('int')\n",
    "    y_test = y_test[test_idxs].astype('int')\n",
    "    X_train = X_train[train_idxs]\n",
    "    X_test = X_test[test_idxs]\n",
    "    y_train = (y_train == Y1).astype('int')\n",
    "    y_test = (y_test == Y1).astype('int')\n",
    "    return (X_train, y_train), (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_mnist_threes_nines()\n",
    "image = X_train[1]\n",
    "display_image(image)\n",
    "print(\"Label:\", y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "############ Naive Sigmoid #####################\n",
    "################################################\n",
    "\n",
    "def sigmoid_activation(s):\n",
    "    out = 1 / (1 + np.exp(-s))\n",
    "    grad = out * (1 - out)\n",
    "    return out, grad\n",
    "\n",
    "s = np.asarray([1., 0., -1])\n",
    "out, grad = sigmoid_activation(s)\n",
    "print(out)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "############ Better Sigmoid ####################\n",
    "################################################\n",
    "def sigmoid_activation(s):\n",
    "    mask = (s >= 0)\n",
    "    out = np.where(mask, \n",
    "                   1 / (1 + np.exp(-s, where=mask)), \n",
    "                   np.exp(s, where=~mask) / (1 + np.exp(s, where=~mask)))\n",
    "    grad = out * (1 - out)\n",
    "    return out, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    mask = (x >= 0)\n",
    "    pos_sig = 1 / (1 + np.exp(-x, where=mask, out=np.ones_like(x)))\n",
    "    neg_sig = np.exp(x, where=~mask, out=np.ones_like(x)) / \\\n",
    "        (1 + np.exp(x, where=~mask, out=np.ones_like(x)))\n",
    "    out = np.where(mask, \n",
    "                   pos_sig, \n",
    "                   neg_sig)\n",
    "    eps = 1e-15\n",
    "    out = np.clip(out, eps, 1. - eps)\n",
    "    grad = out * (1 - out)\n",
    "    return out, grad\n",
    "\n",
    "\n",
    "def logistic_loss(g, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for binary classification with logistic\n",
    "    loss\n",
    "\n",
    "    Inputs:\n",
    "    - g: Output of final layer with sigmoid activation,\n",
    "         of shape (N, 1) \n",
    "\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] \n",
    "         and y[i] in {0, 1}\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: array of losses\n",
    "    - dL_dg: Gradient of the loss with respect to g\n",
    "    \"\"\"\n",
    "    assert(g.ndim == 2 and g.shape[1] == 1)\n",
    "    assert(y.ndim == 1)\n",
    "    g = g.reshape(-1)\n",
    "    loss = -(y * np.log(g) + (1 - y) * np.log(1 - g))\n",
    "    dL_dg = -((y / g) + (y - 1)/(1 - g))\n",
    "    return loss, dL_dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu_activation(s):\n",
    "    mask = s <= 0\n",
    "    out = np.where(mask, 0, s)\n",
    "    ds = np.where(mask, 0.0, 1.0)\n",
    "    return out, ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def layer_forward(x, W, b, activation_fn):\n",
    "    assert(x.ndim == 2)\n",
    "    assert(W.ndim == 2 and W.shape[0] == x.shape[1])\n",
    "    assert(b.ndim == 2 and b.shape[0] == 1 and b.shape[1] == W.shape[1])\n",
    "    h = x @ W + b\n",
    "    out, ds = activation_fn(h)\n",
    "    cache = (x, W, b, ds)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_weight_matrices(layer_dims):\n",
    "    \"\"\"\n",
    "    Creates a list of weight matrices defining the weights of NN\n",
    "    \n",
    "    Inputs:\n",
    "    - layer_dims: A list whose size is the number of layers. layer_dims[i] defines\n",
    "      the number of neurons in the i+1 layer.\n",
    "\n",
    "    Returns a list of weight matrices\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        nrow = layer_dims[i]\n",
    "        ncol = layer_dims[i+1]\n",
    "        W = np.random.randn(nrow, ncol) * 0.01\n",
    "        weights.append(W)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def create_bias_vectors(layer_dims):\n",
    "    \"\"\"\n",
    "    Creates a list of weight matrices defining the weights of NN\n",
    "    \n",
    "    Inputs:\n",
    "    - layer_dims: A list whose size is the number of layers. layer_dims[i] defines\n",
    "      the number of neurons in the i+1 layer.\n",
    "\n",
    "    Returns a list of weight matrices\n",
    "    \"\"\"\n",
    "    biases = [np.random.randn(1, h)* 0.01 for h in layer_dims[1:]]\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(X_batch, weight_matrices, biases, activations):\n",
    "    assert(len(weight_matrices) == len(biases))\n",
    "    assert(len(weight_matrices) == len(activations))\n",
    "    layer_caches = []\n",
    "    h = X_batch\n",
    "    for layer in range(len(weight_matrices)):\n",
    "        W = weight_matrices[layer]\n",
    "        b = biases[layer]\n",
    "        activation_fn = activations[layer]\n",
    "        h, layer_cache = layer_forward(h, W, b, activation_fn)\n",
    "        layer_caches.append(layer_cache)\n",
    "    output = h\n",
    "    return output, layer_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(dL_dg, layer_caches):\n",
    "\n",
    "    grad_Ws = []\n",
    "    grad_bs = []\n",
    "\n",
    "    ##########################################################\n",
    "    ################### Base Case ############################\n",
    "    ##########################################################\n",
    "    layer_caches_reversed = layer_caches[::-1]\n",
    "    x_l_min_1, W_l, b_l, g_prime = layer_caches_reversed[0]\n",
    "    delta_l = dL_dg.reshape(-1, 1) * g_prime\n",
    "\n",
    "    grad_W = np.einsum(\"ni,nj->nij\", x_l_min_1, delta_l).mean(axis=0)\n",
    "    grad_b = delta_l.mean(axis=0, keepdims=True)\n",
    "\n",
    "    grad_Ws.insert(0, grad_W)\n",
    "    grad_bs.insert(0, grad_b)\n",
    "\n",
    "    W_dot_delta = np.einsum(\"ij,nj->ni\", W_l, delta_l)\n",
    "\n",
    "    ##########################################################\n",
    "    ################### Inductive Step #######################\n",
    "    ##########################################################\n",
    "    for layer in layer_caches_reversed[1:]:\n",
    "        x_l_min_1, W_l, b_l, g_prime = layer\n",
    "        delta_l = g_prime * W_dot_delta\n",
    "\n",
    "        grad_W = np.einsum(\"ni,nj->nij\", x_l_min_1, delta_l).mean(axis=0)\n",
    "        grad_b = delta_l.mean(axis=0, keepdims=True)\n",
    "\n",
    "        grad_Ws.insert(0, grad_W)\n",
    "        grad_bs.insert(0, grad_b)\n",
    "\n",
    "        W_dot_delta = np.einsum(\"ij,nj->ni\", W_l, delta_l)\n",
    "    return grad_Ws, grad_bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_dims = [28*28, 200, 1]\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "weight_matrices = create_weight_matrices(layer_dims)\n",
    "biases = create_bias_vectors(layer_dims)\n",
    "bs = 100\n",
    "step_size = 0.1\n",
    "training_stats = []\n",
    "for epoch in range(5):\n",
    "    idxs = np.arange(X_train.shape[0])\n",
    "    idxs = np.random.permutation(idxs)\n",
    "    for batch in range(X_train.shape[0] // bs):\n",
    "        batch_idxs = idxs[batch * bs : (batch + 1) * bs]\n",
    "        X_batch = X_train[batch_idxs]\n",
    "        X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "        y_batch = y_train[batch_idxs]\n",
    "\n",
    "        output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "        loss, dL_dg = logistic_loss(output, y_batch)\n",
    "        grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n",
    "\n",
    "        #######################################################\n",
    "        ################ Update Gradients #####################\n",
    "        #######################################################\n",
    "        for weight_matrix, bias, grad_W, grad_b in zip(weight_matrices, biases, grad_Ws, grad_bs):\n",
    "            weight_matrix -= (step_size * grad_W)\n",
    "            bias -= (step_size * grad_b)\n",
    "\n",
    "        y_hat = (output > 0.5).astype(np.int).reshape(-1)\n",
    "\n",
    "        accuracy = (y_hat == y_batch).sum() / y_batch.shape[0]\n",
    "\n",
    "        avg_train_loss = loss.mean()\n",
    "        avg_train_acc = accuracy\n",
    "\n",
    "\n",
    "        #######################################################\n",
    "        ############### Test Loss #############################\n",
    "        #######################################################\n",
    "        output, _ = forward_pass(X_test.reshape(X_test.shape[0], -1), weight_matrices, biases, activations)\n",
    "        loss, ds = logistic_loss(output, y_test)\n",
    "\n",
    "        y_hat = (output > 0.5).astype(np.int).reshape(-1)\n",
    "        accuracy = (y_hat == y_test).sum() / y_test.shape[0]\n",
    "\n",
    "        avg_test_loss = loss.mean()\n",
    "        avg_test_acc = accuracy\n",
    "\n",
    "        training_stats.append([avg_train_loss, avg_train_acc, avg_test_loss, avg_test_acc])\n",
    "\n",
    "        print(f\"Test Loss: {loss.mean():.3f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Look at loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(training_stats, columns = [\"Train_Loss\", \"Train_Acc\", \"Test_Loss\", \"Test_Acc\"])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.Train_Loss, label=\"Training Loss\")\n",
    "ax.plot(df.Test_Loss, label=\"Test Loss\")\n",
    "ax.set(ylim=(0, 1.))\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"train_test_losses_lr_01.png\", dpi=300)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.Train_Acc, label=\"Training Accuracy\")\n",
    "ax.plot(df.Test_Acc, label=\"Test Accuracy\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"train_test_accuracy_lr_01.png\", dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Look at wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output, _ = forward_pass(X_test.reshape(X_test.shape[0], -1), weight_matrices, biases, activations)\n",
    "loss, dL_dg = logistic_loss(output, y_test)\n",
    "\n",
    "y_hat = (output > 0.5).astype(np.int).reshape(-1)\n",
    "\n",
    "wrong_preds = X_test[y_hat != y_test]\n",
    "wrong_preds_label = y_test[y_hat != y_test]\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "display_image(wrong_preds[i]), wrong_preds_label[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check gradient with numerical differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fd_grad(func, input_array, eps=1e-5):\n",
    "\t\"\"\"\n",
    "\t`func` takes in a (flat!) array and returns a scalar\n",
    "\t`input_array` is assumed to be flat and the correct size for inputting\n",
    "\t\"\"\"\n",
    "\tinput_array = input_array.copy()\n",
    "\tgrad = np.zeros_like(input_array)\n",
    "\tfor i in range(input_array.shape[0]):\n",
    "\t    input_array[i] += eps\n",
    "\t    val1 = func(input_array)\n",
    "\t    input_array[i] -= (2 * eps)\n",
    "\t    val2 = func(input_array)\n",
    "\t    input_array[i] += eps\n",
    "\t    grad[i] = (val1 - val2) / (2 * eps)\n",
    "\treturn grad\n",
    "\n",
    "def backward_pass_checker(X_batch, y_batch, weight_matrices, biases, activations):\n",
    "    grad_Ws, grad_bs = [], []\n",
    "    for i, W in enumerate(weight_matrices):\n",
    "        def checker(W_flat):\n",
    "            weight_matrices[i] = W_flat.reshape(W.shape)\n",
    "            output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "            loss, dL_dg = logistic_loss(output, y_batch)\n",
    "            return loss.mean()\n",
    "        grad_Ws.append(fd_grad(checker, W.reshape(-1)).reshape(W.shape))\n",
    "    for i, b in enumerate(biases):\n",
    "        def checker(b_flat):  # is b already flat?\n",
    "            biases[i] = b_flat.reshape(b.shape)\n",
    "            output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)            \n",
    "            loss, dL_dg = logistic_loss(output, y_batch)\n",
    "            return loss.mean()\n",
    "        grad_bs.append(fd_grad(checker, b.reshape(-1)).reshape(b.shape))\n",
    "    return grad_Ws, grad_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_dims = [28*28, 20, 1]\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "weight_matrices = create_weight_matrices(layer_dims)\n",
    "biases = create_bias_vectors(layer_dims)\n",
    "bs = 100\n",
    "batch_idx = 1\n",
    "X_batch = X_train[batch_idx*bs:(batch_idx+1)*bs]\n",
    "X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "y_batch = y_train[batch_idx*bs:(batch_idx+1)*bs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_Ws_fd, grad_bs_fd = backward_pass_checker(X_batch, y_batch, weight_matrices, biases, activations)\n",
    "\n",
    "for W, W_fd in zip(grad_Ws, grad_Ws_fd):\n",
    "    print(np.abs(W - W_fd).max())\n",
    "    print(np.abs(W - W_fd).mean())\n",
    "\n",
    "for b, b_fd in zip(grad_bs, grad_bs_fd):\n",
    "    print(np.abs(b - b_fd).max())\n",
    "    print(np.abs(b - b_fd).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "grad_Ws, grad_bs = backward_pass_checker(X_batch, y_batch, weight_matrices, biases, activations)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print()\n",
    "    print(grad_bs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, _ = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "print(f\"{loss.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print()\n",
    "    print(grad_bs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/Users/hunternisonoff/anaconda3/envs/ml/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "BackProp_Solutions.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
